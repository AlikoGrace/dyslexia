{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834b48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean as eu\n",
    "from scipy.spatial.distance import cosine \n",
    "import math\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36fcb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Converts the eye-tracking data of the Dyslexic and Control candidates present in the data folder into lists of datafromes\n",
    "#Each data frame represents the data of 1 candidate unlike other datasets that jams them all together\n",
    "#The entire data is converted into 2 lists:\n",
    "# 1. C_data for control candidates \n",
    "# 2. D_data for dyslexic candidates\n",
    "#Structure of the dataframes:\n",
    "#        LX    LY    RX    RY\n",
    "#    0   ..    ..    ..    .. \n",
    "#    1   ..    ..    ..    .. \n",
    "#    2   ..    ..    ..    .. \n",
    "#   ..   ..    ..    ..    .. \n",
    "#    n   ..    ..    ..    .. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    D_path = glob.glob('Data\\Dyslexic' + \"\\*\")#locates all files in that directory no matter where they are located or named\n",
    "    C_path = glob.glob('Data\\Control' + \"\\*\")\n",
    "     \n",
    "    C_data = []#an empty list that would contain datasets for control \n",
    "    for path in C_path:\n",
    "        temp = pd.read_csv(path)#it then reads all the paths and puts it in a temporal variable.\n",
    "        temp = temp.drop('Unnamed: 0',axis = 1)#it then drops the dataset called unamed because it is not part and then puts the \n",
    "        #cleaned one into the c_data list\n",
    "        C_data.append(temp)\n",
    "\n",
    "    D_data = []\n",
    "    for path in D_path:\n",
    "        temp = pd.read_csv(path)\n",
    "        temp = temp.drop('Unnamed: 0',axis = 1)\n",
    "        D_data.append(temp)\n",
    "        #does same as above for the dyslexia datasets\n",
    "\n",
    "    return C_data, D_data\n",
    "    #it then returns both as cleaned list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af226c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Control and Dyslexic data as required for the STFT operations \n",
    "def get_stft_data(C_data, D_data): \n",
    "    C_new = []\n",
    "    for data in C_data:\n",
    "        X =data[['LX','RX']]# separates the x parts for both left and right eye\n",
    "        Y =data[['LY','RY']]#does same here\n",
    "        Xm = X.mean(axis=1)#calculates the mean for the x values(both left and right eye to get a single coordinate representing the average positon)\n",
    "        Ym = Y.mean(axis=1) #does same for Y axis= 1 here means it should calculate the mean for the horizontal axis, 0 is vertical\n",
    "        #f = pd.DataFrame([data.iloc[:,0],Xm,Ym])\n",
    "        f = pd.DataFrame({\n",
    "            'T': data.iloc[:,0].values,\n",
    "            'X': Xm.values,\n",
    "            'Y': Ym.values\n",
    "        }) #T is used to name first column and x for the xmean etc\n",
    "        f = f.transpose() #turns the rows into column and the columns into rows to make up the swapping done at the f part.\n",
    "        f = f.rename(columns = {'Unnamed 0': 'X', 'Unnamed 1': 'Y'})\n",
    "        C_new.append(f)\n",
    "\n",
    "    D_new = []\n",
    "    for data in D_data:\n",
    "        X =data[['LX','RX']]\n",
    "        Y =data[['LY','RY']]\n",
    "        Xm = X.mean(axis=1)\n",
    "        Ym = Y.mean(axis=1)\n",
    "        f = pd.DataFrame([data.iloc[:,0],Xm,Ym])\n",
    "       # f = f.transpose()\n",
    "       # f = f.rename(columns = {'Unnamed 0': 'X', 'Unnamed 1': 'Y'})\n",
    "        D_new.append(f)\n",
    "    \n",
    "    C_new,D_new = normalise_data(C_new,D_new)#scales both tow 1 for comparison purposes\n",
    "    \n",
    "    #How data is preped for STFT, revisit STFT\n",
    "    C_cmx = []\n",
    "    C_real= []\n",
    "    C_img=[]\n",
    "    for j in range(len(C_new)):\n",
    "        dat = C_new[j]\n",
    "        x = dat['X']\n",
    "        y = dat['Y']\n",
    "        t = dat['T']\n",
    "\n",
    "        z=[]\n",
    "        x_in=[]\n",
    "        y_in=[]\n",
    "        for i in range(0,x.size):\n",
    "            z.append(complex(x[i],y[i]))\n",
    "            x_in.append(x[i])\n",
    "            y_in.append(y[i])\n",
    "\n",
    "\n",
    "        C_cmx.append(z)\n",
    "        C_real.append(x_in)\n",
    "        C_img.append(y_in)\n",
    "\n",
    "    D_cmx = []\n",
    "    D_real= []\n",
    "    D_img=[]\n",
    "    for j in range(len(D_new)):\n",
    "        dat = D_new[j]\n",
    "        x = dat['X']\n",
    "        y = dat['Y']\n",
    "        t = dat['T']\n",
    "\n",
    "        z=[]\n",
    "        x_in=[]\n",
    "        y_in=[]\n",
    "        for i in range(0,x.size):\n",
    "            z.append(complex(x[i],y[i]))\n",
    "            x_in.append(x[i])\n",
    "            y_in.append(y[i])\n",
    "        D_cmx.append(z)\n",
    "        D_real.append(x_in)\n",
    "        D_img.append(y_in)\n",
    "    \n",
    "    return C_cmx, C_real, C_img, D_cmx, D_real, D_img, C_new, D_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc3abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization, in this context, scales the data such that the maximum absolute value in each dimension (X or Y) becomes 1. \n",
    "#This is useful for ensuring that the data across all participants is on a similar scale, \n",
    "#important for comparison and analysis.    \n",
    "def normalise_data(C_new,D_new): \n",
    "    for i in range(len(C_new)):\n",
    "        C_tempx = np.abs(C_new[i]['X'])\n",
    "        mx = max(C_tempx)\n",
    "        C_tempy = np.abs(C_new[i]['Y'])\n",
    "        my= max(C_tempy)\n",
    "        C_new[i]['X'] = C_new[i]['X']/np.abs(mx)\n",
    "        C_new[i]['Y'] = C_new[i]['Y']/np.abs(my)\n",
    "    for i in range(len(D_new)):\n",
    "        D_tempx = np.abs(D_new[i]['X'])\n",
    "        mx = max(D_tempx)\n",
    "        D_tempy = np.abs(D_new[i]['Y'])\n",
    "        my= max(D_tempy)\n",
    "        D_new[i]['X'] = D_new[i]['X']/np.abs(mx)\n",
    "        D_new[i]['Y'] = D_new[i]['Y']/np.abs(my)  \n",
    "    return C_new,D_new\n",
    "    \n",
    "    \n",
    "def average_l_r(data):\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Calculates the average values of the left and righty eye readings.\n",
    "# INPUT:\n",
    "#     data   : dataframe of a single candidate \n",
    "\n",
    "#x   : the average of the x coordinates of the left and right eye readings\n",
    "#y   : the average of the y coordinates of the left and right eye readings\n",
    "\n",
    "#OUTPUT:\n",
    "#     x_y_data: combines the average of the x and y coordinates into a dictionary of form: \n",
    "#     X: x,\n",
    "#     Y: y\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    x = [sum(x)/2 for x in zip(data['LX'].to_list(), data['RX'].tolist())]\n",
    "    y = [sum(x)/2 for x in zip(data['LY'].to_list(), data['RY'].tolist())]\n",
    "    x_y_data = {'X':x, 'Y':y}\n",
    "    \n",
    "    return x_y_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81bd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_lens():\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Returns a representation of lengths of the entries in C_data and D_data combined. \n",
    "#OUTPUT: \n",
    "#     lens: contains the representation of lengths of each entry in C_data and D_data in this order.\n",
    "#           value : length represented\n",
    "#             0   :        999\n",
    "#             1   :        1249\n",
    "#             2   :        1499\n",
    "#             3   :        1749\n",
    "#             5   :        1999\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    C_data, D_data = get_data()\n",
    "    lens = []\n",
    "    for dSet in [C_data, D_data]:\n",
    "        for data in dSet:\n",
    "            lens.append(int(((len(data['LX']) + 1)/250) - 4))\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4670c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
